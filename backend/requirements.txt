fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
alembic==1.12.1
pydantic==2.5.0
pydantic-settings==2.1.0
celery==5.3.4
redis==5.0.1
httpx==0.25.2
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
bcrypt==4.0.1
python-multipart==0.0.6
python-dotenv==1.0.0
# Enhanced Security Dependencies
pyotp==2.9.0
cryptography==41.0.7
psutil==5.9.6
requests==2.31.0

# AI Model Dependencies (GGUF format for better performance)
# GGUF primary (using llama-cpp-python)
llama-cpp-python[server]>=0.2.0

# Transformers fallback (legacy support)
transformers>=4.36.0
accelerate>=0.25.0
bitsandbytes>=0.41.3
sentencepiece>=0.1.99
protobuf==3.20.3
scipy>=1.11.4
numpy>=1.24.3

# PyTorch (required for transformers fallback)
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0

# Note: Celery removed - AI model now integrated into backend
# No separate worker needed for real-time chat