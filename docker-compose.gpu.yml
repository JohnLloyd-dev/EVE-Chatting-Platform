services:
  # PostgreSQL Database
  postgres:
    image: postgres:17
    environment:
      POSTGRES_USER: adam2025man
      POSTGRES_PASSWORD: eve@postgres@3241
      POSTGRES_DB: chatting_platform
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U adam2025man"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Redis Cache/Broker
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    environment:
      - DATABASE_URL=postgresql://adam2025man:eve@postgres@3241@postgres:5432/chatting_platform
      - REDIS_URL=redis://redis:6379/0
      - AI_MODEL_URL=http://ai-server:8000
      - AI_MODEL_AUTH_USERNAME=adam
      - AI_MODEL_AUTH_PASSWORD=eve2025
      - ADMIN_USERNAME=admin
      - ADMIN_PASSWORD=adam@and@eve@3241
      - JWT_SECRET_KEY=eve-super-secure-jwt-secret-key-2025-production
    volumes:
      - ./backend:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Celery Worker
  celery-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      - DATABASE_URL=postgresql://adam2025man:eve@postgres@3241@postgres:5432/chatting_platform
      - REDIS_URL=redis://redis:6379/0
      - AI_MODEL_URL=http://ai-server:8000
      - AI_MODEL_AUTH_USERNAME=adam
      - AI_MODEL_AUTH_PASSWORD=eve2025
    volumes:
      - ./backend:/app
    command: celery -A celery_app worker --loglevel=info
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  # AI Server (GPU Version)
  ai-server:
    build:
      context: ./ai_server
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - TRANSFORMERS_CACHE=/app/.cache/huggingface
      - HF_HOME=/app/.cache/huggingface
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=utility,compute
    volumes:
      - ai_model_cache:/app/.cache/huggingface
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 180s
    restart: unless-stopped

  # Next.js Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://204.12.233.105:8001
      - HOSTNAME=0.0.0.0
      - PORT=3000
    volumes:
      - ./frontend:/app
      - /app/node_modules
    command: npm run dev
    depends_on:
      backend:
        condition: service_started
    restart: unless-stopped

volumes:
  postgres_data:
  ai_model_cache:
